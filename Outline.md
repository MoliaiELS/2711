# 2711
---

# **1. 引入：为什么需要交叉熵？**（1分钟）

- **场景提问**：
“如果天气预报说‘明天降雨概率80%’，但实际没下雨——你会觉得预报准吗？如何量化这种‘不准’？”
- **核心概念**：
交叉熵是衡量“预测分布”与“真实分布”差异的工具，是机器学习分类任务的核心损失函数。
- **应用领域**：
图像分类、语言模型、生成对抗网络（GAN）等。

## 文案：

在日常生活中，我们常常会对一些预测的准确性产生疑问。比如，当天气预报说‘明天降雨概率80%’，但实际却没下雨，你是否会质疑预报的准确性？又该如何量化这种‘不准’呢？
在机器学习的领域里，我们面临着类似的挑战。当模型对数据进行预测时，如何衡量预测结果与真实情况之间的差异，成为了我们亟待解决的问题。而交叉熵，正是那个能够精准衡量‘预测分布’与‘真实分布’差异的关键工具。它宛如一把尺子，为机器学习分类任务中的预测准确性提供了量化标准，广泛应用于图像分类、语言模型、生成对抗网络（GAN）等多个领域。

## 视觉设计：

- 场景一：展示一个天气预报的界面，上面显示“明天降雨概率80%”，随后切换到晴朗的天气画面，配上疑惑的音效和观众可能产生的疑问文字：“准吗？”。
- 场景二：屏幕逐渐聚焦到“交叉熵”几个大字，背景是机器学习模型处理数据的动态效果，旁边浮现图像分类、语言模型、GAN等应用领域的关键词和相关图片，如不同类别的图像、语言文本序列、GAN生成的图像等，体现交叉熵的广泛应用。

---

# **2. 基础铺垫：信息量与熵**（2分钟）

- **信息量**（Self-Information）：
    - 公式：$I(A) = -\log P(A)$
    - 直觉：事件越“意外”，信息量越大（如“太阳西升” vs “太阳东升”）。
- **熵**（Entropy）：
    - 公式： $H(X) = -\sum P(x_i) \log P(x_i)$
    - 直观解释：系统的“混乱程度”（如作弊骰子 vs 公平骰子的熵对比）。
    - **动态图示**：骰子概率分布变化时熵的波动。

## 文案：

现在有一枚六面骰子，当你将它掷出，你可能得到 1 到 6 之间的任意一个点数。
对于一个均质的骰子，每一个点数掷出的概率均为 1/6。所以，当你掷出一个点数时，它们所带来的信息量应该也是一样。
但是如果你掷出了 7 点或者 -1 点。哇哦！这可太不可思议了！这些点数根本不可能出现。所以，这些不可能事件的信息量就会大得惊人。

我们从直觉上理解，可以认为：事件的信息量和它发生的概率有反比关系。就像掷骰子，如果一个点数出现的概率很低，那么一旦它真的出现了，就会给你带来更多的新信息和惊讶；相反，如果一个点数大概率会出现，那它真的出现了，也不会给你带来太多新意。那么数学上，我们可以使用 I(A) = 1/P(A)来描述事件的信息量。
但是考虑连续掷了 10 次骰子，每次都掷出了 6 点。这个事件的概率是 (1/6)^10，非常非常小。而每次掷出 6 点的信息量应该是叠加起来的。而单纯的反比关系无法满足这个计算性质，因此我们采用计算性质更好的对数的方式来描述事件的信息量，$I(A) = -\log P(A)$。

而如果我们想描述整个系统的不确定性，该怎么做呢？回想一下你在小学4年级学过的期望概念，我们可以类似地定义一个系统的平均信息量， $H(X) = -\sum P(x_i) \log P(x_i)$ ，而这就是香农熵。
它可以看作是衡量整个系统混乱程度的指标。现在我们使用更加数学的语言给出信息量和熵的概念
考虑样本空间
Ω={x1, x2, … xn}。

I(x_i) = P(x_i)
H(X) = E(I) = -\sum P(x_i) \log P(x_i)

## 视觉设计：

---

# **3. 交叉熵：当你的假设错了**（2分钟）

- **骰子实验**：
    - 真实分布  $P$（作弊骰子：1点概率0.5，其他点0.1）。
    - 你的假设 \( $Q$ \)（以为骰子公平：每面概率1/6）。
- **交叉熵公式**：
\( $H(P, Q) = -\sum P(x_i) \log Q(x_i)$ \)
- **关键对比**：
    - 真实熵  $H(P)$  vs 交叉熵  $H(P, Q)$
    - **动画演示**：Q分布偏离P时，交叉熵如何飙升。

## 文案：

在之前的例子中，我们计算掷骰子的熵是直接使用了这一事件的概率分布，然而在实际问题中，我们往往不知道事件的实际概率分布，或者实际概率分布就是我们需要的。而这时候，我们往往会假设一个分布Q，通过Q去预测事件。还是用掷骰子来距离，假设这个骰子被人做过手脚，导致其实际的概率分布为P，但是你并不知道，依然使用概率分布Q（均匀1/6）去预测掷出的点数，那么你计算出的熵就是
***H*(*P*,*Q*)=−∑*P*(*x*)log*Q*(*x*) ,**

而这，就是交叉熵。交叉熵衡量用错误模型Q编码真实分布P的代价。

- 真实熵H(P)*H*(*P*)≈1.76比特：精确掌握作弊规律所需的信息量
- 交叉熵H(P,Q)*H*(*P*,*Q*)≈2.23比特：因错误假设额外产生的认知税

## 视觉设计：

---

# **4. KL散度：交叉熵的深层含义**（1.5分钟）

- **公式推导**：
 $D_{KL}(P \parallel Q) = H(P, Q) - H(P)$
- **物理意义**：
“用Q描述P需要额外付出的信息代价”。
- **性质**：
    - 非对称性（\( $D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$ \)）。
    - 最小值为0（当且仅当P=Q时成立）。
- **图表对比**：不同分布P和Q的KL散度热力图。

## 文案：

在机器学习中，我们会希望我们的假设模型Q和真实模型P尽可能相同，那么我们就可以使用交叉熵来衡量这两个模型之间的差异，更进一步说，我们可以通过最小化H(P, Q) - H(P)来得到最好的模型分布，而这就是KL散度$D_{KL}(P \parallel Q) = H(P, Q) - H(P)$

KL散度有两个性质：

非负性和非对称性 $D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$。

**KL散度**最小值为0（当且仅当P=Q时成立），这很好理解，而非对称性还是使用掷骰子理解，如果你的认为骰子被做了手脚，也就是假设分布Q不是平均分布，但是实际分布P是平均的1/6，计算真实熵和交叉熵：

## 视觉设计：

---

# **5. 机器学习中的交叉熵：图像生成为例**（1.5分钟）

- **实战意义**：
“优化交叉熵 = 让模型分布Q逼近真实数据分布P”。
- 采样图像的像素分布，与预测的模型拟合，并让交叉熵最小化

## 文案：

在图片生成任务中，当AI试图绘制一只狗时，可能会生成三只眼睛的怪物——这本质上是因为模型预测的像素分布与真实世界产生了偏差。我们通过交叉熵量化这种偏差：将真实图片的百万像素建模为高维概率分布P，模型生成的分布Q通过神经网络参数θ定义。优化过程就是通过找到最小的交叉熵H(P, Q)让Q不断逼近P。你可能会问：为什么不直接优化KL散度？因为KL散度等于交叉熵减去真实熵，而真实世界的熵是固定不变的。通过反向传播算法，系统逐层调整神经网络参数，将像素级的分布误差从输出层反向传导至最底层的神经元。这就是DALL·E和Stable Diffusion的秘密——它们本质上是数十亿像素的分布对齐大师，用交叉熵铸就虚拟与现实的桥梁。

## 视觉设计：

---

# **6. 总结与拓展**（1分钟）

- **核心公式回顾**：
\( $\text{Loss} = H(P_{\text{data}}, Q_{\text{model}})$ \)
- **应用场景**：
    - 分类任务（Softmax + Cross-Entropy）。
    - 生成模型（如VAE、GAN中的分布对齐）。
- **进阶思考P2:**
    
    在本期视频中我们介绍了通过最小化交叉熵来最小化模型和我们采样到的数据分布之间差距的原理，而在机器学习中，大家一定也经常遇到”极大似然估计“这个概念，极大似然估计也是让模型去尽力拟合我们采样到的数据分布，那么这两个概念有什么区别吗？实际上，极大似然估计和最小化交叉熵是完全等价的，只不过前者是统计学上的概念，而后者是信息学上的概念，接下来让我们来推导一下。
    
    让我们从极大似然估计开始：
    
    1. 我们希望我们的模型θ能最大化观测到的数据在我们模型中出现的概率： 
        
        $$
        \theta = \arg\max_{\theta} p(X \mid \theta) \\ = \arg\max_{\theta} \prod_{i=1}^{N} p(x^{(i)} \mid \theta)
        $$
        
    2. 我们对其进行归一化和取log：
        
        $$
        = \arg\max_{\theta} \frac{1}{N} \sum_{i=1}^{N} \log p(x^{(i)} \mid \theta)
        $$
        
    3. 我们的数据x是从真实分布 $P_{data}(x)$中采样得到的，故根据大数定理：(如果你从某个分布中反复独立抽样，样本数量N 足够大时，样本的平均值就会无限接近真实的数学期望)我们可得
        
        $$
        = \arg\max_{\theta} \mathbb{E}_{x \sim p_{\text{data}}(x)} \log p(x \mid \theta)
        $$
        
    4. 展开，并把max改为min后我们可得：
        
        $$
        = \arg\max_{\theta} \int_{x} p_{\text{data}}(x) \log p(x \mid \theta) \, dx \\= \arg\min_{\theta} \int_{x} -p_{\text{data}}(x) \log p(x \mid \theta) \, dx
        $$
        
    
    这时候我们就会惊喜地发现，该不就是交叉熵损失函数吗，故我们在此处就证明了极大似然估计和最小化交叉熵其实完全等价
    
    **（这块的视觉设计直接上公式然后进行推导）**
    

---